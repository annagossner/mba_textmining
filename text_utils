{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXZhX+CkC8Lw/gfcp0ge/M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annagossner/mba_textmining/blob/main/text_utils\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzf7n8olY9Zd",
        "outputId": "330b37e3-090c-43c5-f6d3-aed4b0382327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"from\"\n",
            "ERROR: unknown command \"from\"\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "!pip from autocorrect import spell\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter,OrderedDict\n",
        "from pandas import DataFrame\n",
        "!pip from afinn import Afinn\n",
        "\n",
        "\n",
        "#IN: string, boolean (stem yes or no)\n",
        "#OUT: list of tokens\n",
        "def get_clean_tokens(text, dostem):\n",
        "    text = filter_nonprintable(text)\n",
        "    text = lowercase(text)\n",
        "    text = strip_punctuation(text)\n",
        "    tokens = tokenize_text(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    if dostem:\n",
        "        tokens = lemmatize(tokens)\n",
        "        tokens = stem(tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "#IN: list of tokens and int number of desired terms\n",
        "#OUT: prints, but returns None\n",
        "def print_term_distribution(tokens,n):\n",
        "    termcounter = Counter(tokens)\n",
        "    highest = OrderedDict(termcounter.most_common(n))\n",
        "    lowest = OrderedDict(termcounter.most_common()[:-n - 1:-1])\n",
        "\n",
        "    print('Top:===================')\n",
        "    for t, c in highest.items():\n",
        "        print('%s\\t%i' % (t, c))\n",
        "    print('Bottom:===================')\n",
        "    for t, c in lowest.items():\n",
        "        print('%s\\t%i' % (t, c))\n",
        "\n",
        "def get_top_terms(tokens, n):\n",
        "    termcounter = Counter(tokens)\n",
        "    termcounts = OrderedDict(termcounter.most_common(n))\n",
        "    return termcounts\n",
        "\n",
        "def get_afinn_sentiment(text):\n",
        "    afinn = Afinn()\n",
        "    return afinn.score(text)\n",
        "\n",
        "#IN: directory with text files\n",
        "#OUT: list of strings\n",
        "def get_doc_list(dir):\n",
        "    docs = []\n",
        "    for filename in os.listdir(dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(dir, filename), 'r') as f:\n",
        "                text = f.read().strip()\n",
        "                text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
        "                docs.append(text)\n",
        "    return docs\n",
        "\n",
        "#IN string text\n",
        "#OUT string text\n",
        "def filter_nonprintable(text):\n",
        "    import string\n",
        "    # Get the difference of all ASCII characters from the set of printable characters\n",
        "    nonprintable = set([chr(i) for i in range(128)]).difference(string.printable)\n",
        "    # Use translate to remove all non-printable characters\n",
        "    text = text.translate({ord(character):None for character in nonprintable})\n",
        "    #remove newlines\n",
        "    text = text.rstrip('\\r\\n').replace('\\n', '')\n",
        "    return text\n",
        "#IN string text\n",
        "#OUT string text\n",
        "def strip_punctuation(text):\n",
        "    text = ''.join([ l for l in text if (l not in string.punctuation)])\n",
        "    #curly quotes\n",
        "    text = text.replace(u'\\u201c', '').replace(u'\\u201d', '').replace(u'\\u2019', '')\n",
        "    return text\n",
        "\n",
        "#IN string text\n",
        "#OUT string text\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "#IN string\n",
        "#OUT string\n",
        "def spellcheck(token):\n",
        "    return spell(token)\n",
        "\n",
        "#IN string text\n",
        "#OUT list of words\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "#IN list of words\n",
        "#OUT list of words\n",
        "def stem(tokens):\n",
        "    porter = PorterStemmer()\n",
        "    tokens = [porter.stem(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "#IN list of words\n",
        "#OUT list of words\n",
        "def lemmatize(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = []\n",
        "    for t in tokens:\n",
        "        if t:\n",
        "            lemmatized_tokens.append(\n",
        "                lemmatizer.lemmatize(t, get_wordnet_pos(t)))\n",
        "\n",
        "    return lemmatized_tokens\n",
        "#IN list\n",
        "#OUT list\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def print_doc_term_matrix(doc_list, filename):\n",
        "    vec = CountVectorizer(stop_words=stopwords.words('english'))\n",
        "    #vec = TfidfVectorizer(stop_words=stopwords.words('english'), smooth_idf=False)\n",
        "    X = vec.fit_transform(doc_list)\n",
        "    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
        "    df.to_csv(filename, sep='\\t', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "def get_nouns(tokens):\n",
        "    nouns = []\n",
        "    tags = pos_tag(tokens)\n",
        "    for tag in tags:\n",
        "        if tag[1] == 'NN':\n",
        "            nouns.append(tag[0])\n",
        "    return nouns\n",
        "\n",
        "def get_verbs(tokens):\n",
        "    verbs = []\n",
        "    tags = pos_tag(tokens)\n",
        "    for tag in tags:\n",
        "        if tag[1].startswith('VB'):\n",
        "            verbs.append(tag[0])\n",
        "    return verbs\n",
        "\n",
        "def get_adjectives(tokens):\n",
        "    adjectives = []\n",
        "    tags = pos_tag(tokens)\n",
        "    for tag in tags:\n",
        "        if tag[1].startswith('JJ'):\n",
        "            adjectives.append(tag[0])\n",
        "    return adjectives\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def get_synonyms_and_antonyms(word):\n",
        "    synonyms = []\n",
        "    antonyms = []\n",
        "\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for l in syn.lemmas():\n",
        "            synonyms.append(l.name())\n",
        "            if l.antonyms():\n",
        "                antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "    return set(synonyms), set(antonyms)\n",
        "\n",
        "def get_tense_score(tokens):\n",
        "    score = 0\n",
        "    verb_count = 0\n",
        "    # -1 for each past tense verb\n",
        "    # +1 for each present tense verb\n",
        "    # total is divided by the total number of verbs\n",
        "    pos_tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    for token in pos_tagged_tokens:\n",
        "        treebank_pos = token[1]\n",
        "        if (treebank_pos.startswith('VB')):\n",
        "            verb_count += 1\n",
        "            if treebank_pos.endswith('D') or treebank_pos.endswith('N'):\n",
        "                score -= 1\n",
        "            else:\n",
        "                score += 1\n",
        "\n",
        "    return score/verb_count"
      ]
    }
  ]
}